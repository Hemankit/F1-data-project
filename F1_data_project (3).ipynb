{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5IVHeHU7A0x"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRg6K0W2CH0D"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Change to the directory containing CSV files\n",
        "csv_folder = '/content/drive/MyDrive/F1_data_project'\n",
        "os.chdir(csv_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lq6MsWyBCgpz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "FastLap_df = pd.read_csv('Fastest_Laps.csv')\n",
        "FastLap_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFUkXHusE05w"
      },
      "outputs": [],
      "source": [
        "FastLap_df.drop(columns=['PitOutTime', 'PitInTime'], inplace=True)\n",
        "FastLap_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KdyQIkEIVLG"
      },
      "outputs": [],
      "source": [
        "fastest_lap = FastLap_df.loc[FastLap_df['LapTime'].idxmin()]\n",
        "fastest_lap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R22BIIaIdHy"
      },
      "outputs": [],
      "source": [
        "fastest_lap['DriverNumber']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nfWWd5OIjeK"
      },
      "outputs": [],
      "source": [
        "FastLap_df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QaboAogHcyn"
      },
      "source": [
        "Objectives with EDA:\n",
        "1. Merge/join laps and fastest laps so we can identify drivers distribution of which lap number they tend to have fastest. Identifying patterns in fastest laptime\n",
        "\n",
        "2. Preprocess all the time variables and make them more easier to work with\n",
        "\n",
        "3. Decide how we are going to deal with telemetry data which seems to be very sparse\n",
        "\n",
        "5. Observe Weather conditions for each of the laps and then look at conditions for fastest laps\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wwvdp04zq188"
      },
      "outputs": [],
      "source": [
        "lap_df = pd.read_csv('Laps.csv')\n",
        "lap_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdyVTEFvJRT2"
      },
      "outputs": [],
      "source": [
        "lap_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6w_JjvSLzlp"
      },
      "outputs": [],
      "source": [
        "lap_data = pd.concat([FastLap_df, lap_df], axis=0, ignore_index=True)\n",
        "lap_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MykX8pZDLrZG"
      },
      "outputs": [],
      "source": [
        "#lap_data['Time'] = pd.to_timedelta(lap_data['Time'])\n",
        "lap_data['LapTime'] = pd.to_timedelta(lap_data['LapTime'])\n",
        "lap_data['normalized_t'] = lap_data['LapTime'].dt.total_seconds()\n",
        "lap_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AY0j4lSM6QJ"
      },
      "outputs": [],
      "source": [
        "lap_data['DriverNumber'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJIN7qrKJhL3"
      },
      "outputs": [],
      "source": [
        "lap_data['Compound'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjeU9qUn5-pG"
      },
      "outputs": [],
      "source": [
        "lap_data['TrackStatus'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THqZp83PS637"
      },
      "outputs": [],
      "source": [
        "def fast_lapnum(subdf):\n",
        "  # Convert 'LapTime' to numeric (total seconds) before finding the minimum\n",
        "  subdf['LapTime'] = pd.to_timedelta(subdf['LapTime'])  # Ensure 'LapTime' is Timedelta\n",
        "  subdf['LapTimeSeconds'] = subdf['LapTime'].dt.total_seconds() # Create a new column with LapTime in seconds\n",
        "  return subdf.loc[subdf['LapTimeSeconds'].idxmin()] # Find the minimum based on the numeric column\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjKLElEuO2Fn"
      },
      "outputs": [],
      "source": [
        "# subdataframes for each driver\n",
        "driver_subgroup = lap_data.groupby('DriverNumber')\n",
        "for driver, driver_subdf in driver_subgroup:\n",
        "  # Create a copy of the sub-DataFrame to avoid modifying the original\n",
        "  temp_subdf = driver_subdf.copy()\n",
        "\n",
        "  # Recalculate normalized_t for the temporary sub-DataFrame\n",
        "  temp_subdf['LapTime'] = pd.to_timedelta(temp_subdf['LapTime'])  # Ensure 'LapTime' is Timedelta\n",
        "  temp_subdf['normalized_t'] = temp_subdf['LapTime'].dt.total_seconds()\n",
        "\n",
        "  fast_laptime = fast_lapnum(temp_subdf) # Pass the copy to fast_lapnum\n",
        "\n",
        "  # Use temp_subdf for plotting since it now contains 'normalized_t'\n",
        "  plt.plot(temp_subdf['LapNumber'], temp_subdf['normalized_t'], label=f'Driver {driver}')\n",
        "  plt.scatter(fast_laptime['LapNumber'], fast_laptime['normalized_t'], color='red', label=f'Fastest Laptime for Driver {driver}')\n",
        "  plt.xlabel('Lap Number')\n",
        "  plt.ylabel('Normalized Lap Time')\n",
        "  plt.title(f'Lap Times for Driver {driver}')\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5-WCBAepbJF"
      },
      "outputs": [],
      "source": [
        "# subdataframes for each driver\n",
        "event_driver_group = lap_data.groupby(['EventName', 'DriverNumber'])\n",
        "for (event, driver) , group_data in event_driver_group:\n",
        "  # Create a copy of the sub-DataFrame to avoid modifying the original\n",
        "  temp_subdf = driver_subdf.copy()\n",
        "\n",
        "  # Recalculate normalized_t for the temporary sub-DataFrame\n",
        "  temp_subdf['LapTime'] = pd.to_timedelta(temp_subdf['LapTime'])  # Ensure 'LapTime' is Timedelta\n",
        "  temp_subdf['normalized_t'] = temp_subdf['LapTime'].dt.total_seconds()\n",
        "\n",
        "  fast_laptime = fast_lapnum(temp_subdf) # Pass the copy to fast_lapnum\n",
        "\n",
        "  # Use temp_subdf for plotting since it now contains 'normalized_t'\n",
        "  plt.plot(temp_subdf['LapNumber'], temp_subdf['normalized_t'], label=f'Driver {driver}')\n",
        "  plt.scatter(fast_laptime['LapNumber'], fast_laptime['normalized_t'], color='red', label=f'Fastest Laptime for Driver {driver}')\n",
        "  plt.xlabel('Lap Number')\n",
        "  plt.ylabel('Normalized Lap Time')\n",
        "  plt.title(f'Lap Times for Driver {driver} at {event}')\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L202XyFR8vP"
      },
      "outputs": [],
      "source": [
        "def fast_lapnum(subdf):\n",
        "  # Convert 'LapTime' to numeric (total seconds) before finding the minimum\n",
        "  subdf['LapTime'] = pd.to_timedelta(subdf['LapTime'])  # Ensure 'LapTime' is Timedelta\n",
        "  subdf['LapTimeSeconds'] = subdf['LapTime'].dt.total_seconds() # Create a new column with LapTime in seconds\n",
        "\n",
        "  # Handle potential NaN values in 'LapTimeSeconds'\n",
        "  if pd.isnull(subdf['LapTimeSeconds'].idxmin()):\n",
        "    return None  # Or raise an exception, or return a default value\n",
        "  else:\n",
        "    return subdf.loc[subdf['LapTimeSeconds'].idxmin()] # Find the minimum based on the numeric column\n",
        "\n",
        "# subdataframes for each driver in each event\n",
        "event_driver_group = lap_data.groupby(['EventName', 'DriverNumber'])\n",
        "for (event, driver), group_data in event_driver_group:\n",
        "\n",
        "  temp_subdf = group_data.copy()\n",
        "\n",
        "  # Recalculate normalized_t for the temporary sub-DataFrame\n",
        "  temp_subdf['LapTime'] = pd.to_timedelta(temp_subdf['LapTime'])  # Ensure 'LapTime' is Timedelta\n",
        "  temp_subdf['normalized_t'] = temp_subdf['LapTime'].dt.total_seconds()\n",
        "\n",
        "  fast_laptime = fast_lapnum(temp_subdf) # Pass the copy to fast_lapnum\n",
        "\n",
        "  # Check if fast_laptime is None before plotting\n",
        "  if fast_laptime is not None:\n",
        "    plt.plot(temp_subdf['LapNumber'], temp_subdf['normalized_t'], label=f'Driver {driver}')\n",
        "    plt.scatter(fast_laptime['LapNumber'], fast_laptime['normalized_t'], color='red', label=f'Fastest Laptime for Driver {driver}')\n",
        "    plt.xlabel('Lap Number')\n",
        "    plt.ylabel('Normalized Lap Time')\n",
        "    plt.title(f'Lap Times for Driver {driver} at {event}')\n",
        "    plt.show()\n",
        "  else:\n",
        "    print(f\"No valid lap times found for Driver {driver} at {event}\") # Print a message or handle the case as needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdC_EFQhPmEB"
      },
      "outputs": [],
      "source": [
        "# Convert the 'LapTime' column to timedelta, coerce errors to NaT\n",
        "lap_data['LapTime'] = pd.to_timedelta(lap_data['LapTime'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaT (Not a Time) values in the 'LapTime' column\n",
        "lap_data = lap_data.dropna(subset=['LapTime'])\n",
        "\n",
        "# Now can find the fastest and slowest lap times\n",
        "fastest_fastlap = lap_data.loc[lap_data['LapTime'].idxmin()]\n",
        "slowest_fastlap = lap_data.loc[lap_data['LapTime'].idxmax()]\n",
        "fastest_fastlap, slowest_fastlap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXoqGCtQYrxF"
      },
      "source": [
        "1. fastest_fastlap (the absolute fastest lap):\n",
        "\n",
        "Benchmark: Represents the ultimate performance limit observed within the dataset. It can serve as a benchmark to compare other laps or drivers against.\n",
        "\n",
        "Track Conditions and Driver Skill: Provides insights into the ideal combination of track conditions and driver skill that led to this exceptional lap time. Analyzing the associated data points (driver, lap number, weather conditions, etc.) can help identify key factors contributing to peak performance.\n",
        "\n",
        "Outlier Detection: If the fastest_fastlap significantly deviates from other fast laps, it could indicate an outlier or an unusual event during the race (e.g., a sudden improvement in track conditions). This requires further investigation.\n",
        "\n",
        "2. slowest_fastlap (the slowest among fastest laps):\n",
        "\n",
        "Performance Variation: Highlights the variability in fastest lap times across drivers or race conditions. It indicates the lower end of the performance spectrum for \"fastest\" laps.\n",
        "\n",
        "Identifying Limiting Factors: By analyzing the data points associated with the slowest_fastlap (driver, lap number, weather, tire compound, etc.), you can potentially identify factors that limited performance in that particular instance (e.g., tire degradation, traffic, unfavorable weather).\n",
        "\n",
        "Strategy Insights: In some cases, a slower \"fastest\" lap might be a strategic choice, such as conserving tires or managing fuel. Comparing this lap to other laps by the same driver can provide context and insight into their race strategy.\n",
        "\n",
        "Further Analysis\n",
        "Here are some ways you can use fastest_fastlap and slowest_fastlap in your analysis:\n",
        "\n",
        "Normalization: You can normalize other lap times relative to the fastest_fastlap to get a sense of their performance as a percentage of the best possible time.\n",
        "Performance Distribution: Compare the distribution of all fastest lap times to these two extremes to understand the range and central tendency of performance.\n",
        "Correlation Analysis: Investigate how factors like weather, tire compound, and driver experience correlate with the lap times, focusing on how these factors differ between fastest_fastlap and slowest_fastlap.\n",
        "Segmentation: Group drivers or laps based on their proximity to fastest_fastlap or slowest_fastlap to identify performance clusters or patterns.\n",
        "Predictive Modeling: Use the information gleaned from these extreme laps as features in predictive models to forecast lap times or race outcomes.\n",
        "By understanding and comparing these extreme cases, you can gain a more comprehensive understanding of the factors that influence performance in Formula 1 racing and use this knowledge for more detailed analysis and predictions. I hope this explanation is helpful!\n",
        "\n",
        "Rate this answer\n",
        "Yes this is the essence of the goal for this particular project. I definitely want to try some of these analyses further and I believe I am going in a right path.\n",
        "\n",
        "That's excellent to hear! It sounds like you've got a solid grasp of the objectives and have a good plan for using fastest_fastlap and slowest_fastlap in your analysis.\n",
        "\n",
        "I'm confident that exploring the ideas we discussed, such as normalization, performance distribution analysis, correlation analysis, and segmentation, will lead to valuable insights. Remember to keep in mind the context of these extreme laps and their potential strategic implications within a race.\n",
        "\n",
        "For example, when analyzing performance distribution, you might consider creating visualizations (histograms, box plots) to see how the other fastest lap times cluster around these two extremes. This can help you understand the typical range of \"fastest\" lap times and identify potential outliers.\n",
        "\n",
        "In your correlation analysis, don't hesitate to explore interactions between different factors. For instance, how does the impact of tire compound on lap times change depending on the weather conditions?\n",
        "\n",
        "When segmenting drivers or laps, consider using clustering algorithms or simply grouping them based on quantiles of lap time relative to fastest_fastlap and slowest_fastlap. This can reveal interesting patterns and performance tiers.\n",
        "\n",
        "And as you move towards predictive modeling, you can use the information from these extreme laps, along with other relevant features, to train models that predict lap times or race outcomes.\n",
        "\n",
        "Remember to document your findings and interpretations clearly. This will help you build a strong narrative around your analysis and communicate your insights effectively.\n",
        "\n",
        "I'm excited to see what you discover in your project! Feel free to reach out if you have any further questions or need guidance along the way. I'm here to support your exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy8PMf38ass7"
      },
      "outputs": [],
      "source": [
        "# Outlier analysis\n",
        "# boxplot of all fastest lap times\n",
        "sns.boxplot(x=lap_data['LapTime'])\n",
        "plt.title('Boxplot of All Fastest Lap Times')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWuUJMe9oe2s"
      },
      "outputs": [],
      "source": [
        "# calculating z-score of fastest_fastlap\n",
        "from scipy import stats\n",
        "\n",
        "# Get all lap times as a NumPy array, handling potential missing values\n",
        "lap_times = lap_data['LapTime'].dt.total_seconds().values\n",
        "\n",
        "# Remove NaN values before calculating z-scores\n",
        "valid_lap_times = lap_times[~np.isnan(lap_times)]\n",
        "\n",
        "# Calculate z-scores for valid lap times\n",
        "z_scores = np.abs(stats.zscore(valid_lap_times))\n",
        "\n",
        "# Get the index of the fastest lap within the original lap_times array\n",
        "fastest_lap_index = lap_data['LapTime'].idxmin()\n",
        "\n",
        "# Check if the fastest lap time is valid (not NaN)\n",
        "if not np.isnan(lap_times[fastest_lap_index]):\n",
        "  # Adjust the index to match the valid_lap_times array\n",
        "  adjusted_index = np.where(lap_times == lap_times[fastest_lap_index])[0][0]\n",
        "  # Get the z-score for the fastest lap\n",
        "  fastest_lap_zscore = z_scores[adjusted_index - np.sum(np.isnan(lap_times[:adjusted_index]))]\n",
        "else:\n",
        "  # Handle the case where the fastest lap time is NaN\n",
        "  fastest_lap_zscore = np.nan # or raise an exception\n",
        "\n",
        "fastest_lap_zscore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyyGDDcR0VzZ"
      },
      "source": [
        "This value supports the observation that there was no outlier visible in the boxplot to the left of the boxplot representing lower laptime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0SKe6EnG0C13"
      },
      "outputs": [],
      "source": [
        "# Adressing outliers to the left of the box plot (high laptime)\n",
        "Q1 = lap_data['LapTime'].quantile(0.25)\n",
        "Q3 = lap_data['LapTime'].quantile(0.75)\n",
        "Q2 = lap_data['LapTime'].quantile(0.5)\n",
        "IQR = Q3 - Q1\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "slow_outliers = lap_data[(lap_data['LapTime'] < lower_bound) | (lap_data['LapTime'] > upper_bound)]\n",
        "non_slow_outliers = lap_data[(lap_data['LapTime'] >= Q2) & (lap_data['LapTime'] <= upper_bound)]\n",
        "print(f\"Number of outliers: {len(slow_outliers)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UdeQq6PrTO_B"
      },
      "outputs": [],
      "source": [
        "PCA_df = lap_data.copy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "PCA_df.drop(columns=['Time', 'DriverNumber', 'LapStartTime', 'Team', 'Driver', 'IsAccurate', 'RoundNumber', 'EventName', 'PitOutTime', 'PitInTime', 'LapStartDate'], inplace=True)\n",
        "\n",
        "\n",
        "# preprocessing steps\n",
        "PCA_df['FreshTyre'] = PCA_df['FreshTyre'].astype(int)\n",
        "PCA_df['IsPersonalBest'] = PCA_df['IsPersonalBest'].astype(int)\n",
        "# non binary\n",
        "compound_mapping = {'SOFT' : 0, 'MEDIUM': 1, 'HARD' : 2, 'INTERMEDIATE' : 3, 'WET' : 4}\n",
        "PCA_df['Compound'] = PCA_df['Compound'].map(compound_mapping)\n",
        "\n",
        "# trackstatus\n",
        "track_status_map = {\n",
        "        1: 'Green',\n",
        "        2: 'Yellow',\n",
        "        4: 'Red',\n",
        "        5: 'SC',        # Safety Car\n",
        "        6: 'VSC',       # Virtual Safety Car\n",
        "        7: 'Double Yellow'\n",
        "    }\n",
        "PCA_df['TrackStatusCat'] = PCA_df['TrackStatus'].map(track_status_map).fillna('Mixed')\n",
        "PCA_df = pd.get_dummies(PCA_df, columns=['TrackStatusCat'], prefix='Track')\n",
        "#print(\"PCA_df.shape after one-hot encoding:\", PCA_df.shape)\n",
        "\n",
        "# ... (more preprocessing code) ...\n",
        "# Convert 'object' type columns to numeric\n",
        "object_columns = PCA_df.select_dtypes(include=['object']).columns\n",
        "for col in object_columns:\n",
        "    try:\n",
        "        # Attempt to convert to timedelta, then to total seconds\n",
        "        PCA_df[col] = pd.to_timedelta(PCA_df[col]).dt.total_seconds()\n",
        "    except (ValueError, TypeError):\n",
        "        # If timedelta conversion fails, try converting to numeric directly\n",
        "        try:\n",
        "            PCA_df[col] = pd.to_numeric(PCA_df[col], errors='coerce')\n",
        "            # You might need to handle non-numeric values here (e.g., imputation or dropping rows)\n",
        "        except ValueError:\n",
        "            print(f\"Column '{col}' could not be converted to numeric. Consider other methods.\")\n",
        "\n",
        "# Also, convert 'LapTime' to total seconds\n",
        "PCA_df['LapTime'] = pd.to_timedelta(PCA_df['LapTime']).dt.total_seconds()\n",
        "print(\"Missing values after all encoding and conversions:\\n\", PCA_df.isnull().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hp54DgXnZ8uk"
      },
      "source": [
        "PCA does not work if there are any missing values so needed to impute the means for speed features and compound which seemed to have all values missing due to the duplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpO4NCqQZY9Y"
      },
      "outputs": [],
      "source": [
        "# 1. Fix the Compound mapping\n",
        "# Get unique values in the 'Compound' column of the original lap_data\n",
        "unique_compounds = lap_data['Compound'].unique()\n",
        "\n",
        "# Update the compound_mapping to include all unique values\n",
        "compound_mapping = {compound: i for i, compound in enumerate(unique_compounds)}\n",
        "\n",
        "# Re-apply the mapping\n",
        "PCA_df['Compound'] = lap_data['Compound'].map(compound_mapping)\n",
        "\n",
        "\n",
        "# 2. Handle missing values in other columns:\n",
        "# Option 1: Imputation\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='mean')  # Or 'median', or another strategy\n",
        "PCA_df[['SpeedI1', 'SpeedFL', 'SpeedST', 'Sector1Time', 'Sector1SessionTime']] = imputer.fit_transform(PCA_df[['SpeedI1', 'SpeedFL', 'SpeedST', 'Sector1Time', 'Sector1SessionTime']])\n",
        "\n",
        "# Option 2: Removal (if data loss is acceptable)\n",
        "# PCA_df.dropna(subset=['SpeedI1', 'SpeedFL', 'SpeedST', 'Sector1Time', 'Sector1SessionTime'], inplace=True)\n",
        "\n",
        "\n",
        "# # 3. Remove duplicate columns\n",
        "# # Get a list of all column names\n",
        "# columns = PCA_df.columns.tolist()\n",
        "\n",
        "# # Find and remove duplicate column names (keeping the first occurrence)\n",
        "# seen_columns = set()\n",
        "# columns_to_keep = []\n",
        "# for column in columns:\n",
        "#     if column not in seen_columns:\n",
        "#         seen_columns.add(column)\n",
        "#         columns_to_keep.append(column)\n",
        "\n",
        "# # Select only the unique columns\n",
        "# PCA_df = PCA_df[columns_to_keep]\n",
        "print(PCA_df.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLN_KTV2UJbu"
      },
      "outputs": [],
      "source": [
        "print(PCA_df.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TceZAItXse1"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming PCA_df is your preprocessed DataFrame with all numerical columns\n",
        "\n",
        "# 1. Create a PCA object\n",
        "pca = PCA()  # You can specify n_components if you want to reduce to a specific number of components\n",
        "\n",
        "# 2. Fit the PCA model to your data\n",
        "pca.fit(PCA_df)\n",
        "\n",
        "# 3. Get the principal components\n",
        "principal_components = pca.transform(PCA_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k33Pz6tEk57E"
      },
      "outputs": [],
      "source": [
        "# Checking the ideal n_components using the explained variance ratio\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjtUzS8vdGaU"
      },
      "source": [
        "## Want n_components to be 5 according to the Explained variance ratio plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89B0TSwPdP9L"
      },
      "outputs": [],
      "source": [
        "pca5 = PCA(n_components=5)\n",
        "principal_components5 = pca5.fit_transform(PCA_df)\n",
        "loadings_df = pd.DataFrame(pca5.components_.T, columns=[f'PC{i+1}' for i in range(5)], index=PCA_df.columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khJolh8MeIFR"
      },
      "outputs": [],
      "source": [
        "# Get the component loadings for PC1\n",
        "pc1_loadings = loadings_df['PC1'].abs().sort_values(ascending=False)\n",
        "\n",
        "# Select the top 5 features\n",
        "top_5_features = pc1_loadings.head(5).index.tolist()\n",
        "\n",
        "# Create the new DataFrame with selected features\n",
        "new_df = PCA_df[top_5_features]\n",
        "\n",
        "# Display the first few rows of the new DataFrame\n",
        "print(new_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-P1TaIlm2F9"
      },
      "outputs": [],
      "source": [
        "slow_outliers_pca = slow_outliers[new_df.columns]\n",
        "non_slow_outliers_pca = non_slow_outliers[new_df.columns]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "slow_outliers_pca['Sector1SessionTime'] = pd.to_timedelta(slow_outliers_pca['Sector1SessionTime']).dt.total_seconds()\n",
        "slow_outliers_pca['Sector2SessionTime'] = pd.to_timedelta(slow_outliers_pca['Sector2SessionTime']).dt.total_seconds()\n",
        "slow_outliers_pca['Sector3SessionTime'] = pd.to_timedelta(slow_outliers_pca['Sector3SessionTime']).dt.total_seconds()\n",
        "non_slow_outliers_pca['Sector1SessionTime'] = pd.to_timedelta(non_slow_outliers_pca['Sector1SessionTime']).dt.total_seconds()\n",
        "non_slow_outliers_pca['Sector2SessionTime'] = pd.to_timedelta(non_slow_outliers_pca['Sector2SessionTime']).dt.total_seconds()\n",
        "non_slow_outliers_pca['Sector3SessionTime'] = pd.to_timedelta(non_slow_outliers_pca['Sector3SessionTime']).dt.total_seconds()\n",
        "new_df['Sector1SessionTime'] = pd.to_timedelta(new_df['Sector1SessionTime']).dt.total_seconds()\n",
        "new_df['Sector2SessionTime'] = pd.to_timedelta(new_df['Sector2SessionTime']).dt.total_seconds()\n",
        "new_df['Sector3SessionTime'] = pd.to_timedelta(new_df['Sector3SessionTime']).dt.total_seconds()"
      ],
      "metadata": {
        "id": "xQaLbZ8MHeEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvoxLPxWevol"
      },
      "outputs": [],
      "source": [
        "# Creating a dynamic clustering function to compare outlier and non outlier with respect to the 5 features above\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.mixture import GaussianMixture\n",
        "# generating all the feature combinations from which clusters will be created\n",
        "def feature_combinations(new_df, max_combination_size):\n",
        "  \"\"\"Generates combinations of features from a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df: The pandas DataFrame containing the features.\n",
        "        max_combination_size: The maximum size of feature combinations\n",
        "                               (default: 2, for pairs of features).\n",
        "\n",
        "    Returns:\n",
        "        A list of feature combinations (tuples).\n",
        "    \"\"\"\n",
        "  features = new_df.columns.tolist()\n",
        "  combinations = []\n",
        "  for r in range(1, min(max_combination_size + 1, len(features) + 1)):\n",
        "      combinations.extend(itertools.combinations(features, r))\n",
        "  return combinations\n",
        "\n",
        "def dynamic_clustering(cluster_type):\n",
        "  if cluster_type==Km_clustering:\n",
        "    for combination in feature_combinations(new_df, max_combination_size=2):\n",
        "      # preprocess data\n",
        "      scaler = StandardScaler()\n",
        "      all_data = pd.concat([slow_outliers[list(combination)], non_slow_outliers[list(combination)]])\n",
        "      all_data_scaled = scaler.fit_transform(all_data) # Fit on combined data\n",
        "\n",
        "      Km_clustering = KMeans(n_clusters=2)  # Specify n_clusters=2\n",
        "      all_data_clustered = Km_clustering.fit_predict(all_data_scaled)\n",
        "      slow_outliers['cluster'] = all_data_clustered[:len(slow_outliers)]\n",
        "      non_slow_outliers['cluster'] = all_data_clustered[len(slow_outliers):]\n",
        "      # visualizing the cluster\n",
        "       # Visualize clusters\n",
        "       if len(combination) == 1:\n",
        "          # Check if there's only one feature\n",
        "        feature = combination[0]  # Get the feature name\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.hist(slow_outliers[feature], bins=20, alpha=0.5, label='Slow Outliers', color='red')\n",
        "        plt.hist(non_slow_outliers[feature], bins=20, alpha=0.5, label='Non-Slow Outliers', color='blue')\n",
        "        plt.title(f'Kmeans Distribution of {feature} for Outliers and Non-Outliers')\n",
        "        plt.xlabel(feature)\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.legend()\n",
        "        cluster_centers = km_clustering.cluster_centers_\n",
        "        plt.axvline(cluster_centers[0, 0], color='black', linestyle='--', label = 'cluster center')\n",
        "        plt.axvline(cluster_centers[1, 0], color='black', linestyle='--', label = 'cluster center')\n",
        "        plt.show()\n",
        "\n",
        "      else:\n",
        "\n",
        "        plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
        "\n",
        "      # Scatter plot for slow outliers\n",
        "        plt.scatter(slow_outliers[slow_outliers['cluster'] == 0][list(combination)[0]],\n",
        "                    slow_outliers[slow_outliers['cluster'] == 0][list(combination)[1]],\n",
        "                    label='Slow Outliers Cluster 0', marker='o', color='red', alpha=0.7)\n",
        "        plt.scatter(slow_outliers[slow_outliers['cluster'] == 1][list(combination)[0]],\n",
        "                    slow_outliers[slow_outliers['cluster'] == 1][list(combination)[1]],\n",
        "                    label='Slow Outliers Cluster 1', marker='x', color='red', alpha=0.7)\n",
        "\n",
        "      # Scatter plot for non-slow outliers\n",
        "        plt.scatter(non_slow_outliers[non_slow_outliers['cluster'] == 0][list(combination)[0]],\n",
        "                    non_slow_outliers[non_slow_outliers['cluster'] == 0][list(combination)[1]],\n",
        "                    label='Non-Slow Outliers Cluster 0', marker='o', color='blue', alpha=0.7)\n",
        "        plt.scatter(non_slow_outliers[non_slow_outliers['cluster'] == 1][list(combination)[0]],\n",
        "                    non_slow_outliers[non_slow_outliers['cluster'] == 1][list(combination)[1]],\n",
        "                    label='Non-Slow Outliers Cluster 1', marker='x', color='blue', alpha=0.7)\n",
        "        cluster_centers = km_clustering.cluster_centers_\n",
        "        plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker='*', s=200, c='black', label='Cluster centers')\n",
        "        plt.title(f'Kmeans Clustering for {list(combination)[0]} and {list(combination)[1]}')\n",
        "        plt.xlabel(list(combination)[0])\n",
        "        plt.ylabel(list(combination)[1])\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "  elif cluster_type == gmm:\n",
        "    for combination in feature_combinations(new_df, max_combination_size=2):\n",
        "\n",
        "      all_data = pd.concat([slow_outliers[list(combination)], non_slow_outliers[list(combination)]])\n",
        "      all_data_scaled = scaler.fit_transform(all_data) # Fit on combined data\n",
        "      gmm = GaussianMixture(n_components=2, random_state=42)\n",
        "      gmm.fit(all_data_scaled)\n",
        "      cluster_labels = gmm.predict(all_data_scaled)\n",
        "      slow_outliers['cluster'] = cluster_labels[:len(slow_outliers)]\n",
        "      non_slow_outliers['cluster'] = cluster_labels[len(slow_outliers):]\n",
        "        # visualizing the cluster\n",
        "        if len(combination) == 1:\n",
        "            # Check if there's only one feature\n",
        "          feature = combination[0]  # Get the feature name\n",
        "          plt.figure(figsize=(8, 6))\n",
        "          plt.hist(slow_outliers[feature], bins=20, alpha=0.5, label='Slow Outliers', color='red')\n",
        "          plt.hist(non_slow_outliers[feature], bins=20, alpha=0.5, label='Non-Slow Outliers', color='blue')\n",
        "          plt.title(f'Distribution of {feature} for Outliers and Non-Outliers')\n",
        "          plt.xlabel(feature)\n",
        "          plt.ylabel('Frequency')\n",
        "          plt.legend()\n",
        "          cluster_centers = gmm.means_\n",
        "          plt.axvline(cluster_centers[0, 0], color='black', linestyle='--', label = 'cluster center')\n",
        "          plt.axvline(cluster_centers[1, 0], color='black', linestyle='--', label = 'cluster center')\n",
        "          plt.show()\n",
        "        else:\n",
        "\n",
        "          plt.figure(figsize=(8, 6))  # Adjust figure size as needed\n",
        "          plt.scatter(slow_outliers[slow_outliers['cluster'] == 0][list(combination)[0]],\n",
        "                      slow_outliers[slow_outliers['cluster'] == 0][list(combination)[1]],\n",
        "                      label='Slow Outliers Cluster 0', marker='o', color='red', alpha=0.7)\n",
        "          plt.scatter(slow_outliers[slow_outliers['cluster'] == 1][list(combination)[0]],\n",
        "                      slow_outliers[slow_outliers['cluster'] == 1][list(combination)[1]],\n",
        "                      label='Slow Outliers Cluster 1', marker='x', color='red', alpha=0.7)\n",
        "          plt.scatter(non_slow_outliers[non_slow_outliers['cluster'] == 0][list(combination)[0]],\n",
        "                      non_slow_outliers[non_slow_outliers['cluster'] == 0][list(combination)[1]],\n",
        "                      label='Non-Slow Outliers Cluster 0', marker='o', color='blue', alpha=0.7)\n",
        "          plt.scatter(non_slow_outliers[non_slow_outliers['cluster'] == 1][list(combination)[0]],\n",
        "                      non_slow_outliers[non_slow_outliers['cluster'] == 1][list(combination)[1]],\n",
        "                      label='Non-Slow Outliers Cluster 1', marker='x', color='blue', alpha=0.7)\n",
        "          cluster_centers = gmm.means_\n",
        "          plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker='*', s=200, c='black', label='Cluster centers')\n",
        "          plt.title(f'Gaussian Mixture Clusters for {list(combination)[0]} and {list(combination)[1]}')\n",
        "          plt.xlabel(list(combination)[0])\n",
        "          plt.ylabel(list(combination)[1])\n",
        "          plt.legend()\n",
        "          plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLF3liINPxTW"
      },
      "source": [
        "Asked Gemini to fix the above code and make necessary improvements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JoXLoTzP5o6"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np # Might be needed for creating dummy data\n",
        "\n",
        "# Assuming 'new_df' is the dataframe containing the 5 features you want to use\n",
        "# And 'slow_outliers', 'non_slow_outliers' are subsets of this dataframe or derived from it.\n",
        "# These will need to be defined before calling dynamic_clustering.\n",
        "\n",
        "# Dummy Data Creation (for demonstration purposes - replace with your actual data loading)\n",
        "# data = {f'feature_{i}': np.random.rand(100) for i in range(5)}\n",
        "# data['is_outlier'] = np.random.randint(0, 2, 100) # Example outlier flag\n",
        "# df_full = pd.DataFrame(data)\n",
        "# slow_outliers = df_full[df_full['is_outlier'] == 1].drop(columns=['is_outlier']).copy()\n",
        "# non_slow_outliers = df_full[df_full['is_outlier'] == 0].drop(columns=['is_outlier']).copy()\n",
        "# new_df = df_full.drop(columns=['is_outlier']).copy() # new_df representing the features to choose from\n",
        "\n",
        "def feature_combinations(df, max_combination_size=2):\n",
        "    \"\"\"Generates combinations of features from a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df: The pandas DataFrame containing the features.\n",
        "        max_combination_size: The maximum size of feature combinations.\n",
        "\n",
        "    Returns:\n",
        "        A list of feature combinations (tuples).\n",
        "    \"\"\"\n",
        "    features = df.columns.tolist()\n",
        "    combinations = []\n",
        "    # Ensure max_combination_size doesn't exceed the number of features\n",
        "    for r in range(1, min(max_combination_size + 1, len(features) + 1)):\n",
        "        combinations.extend(itertools.combinations(features, r))\n",
        "    return combinations\n",
        "\n",
        "def plot_clusters(df_outliers, df_non_outliers, combination, clusterer, scaler, cluster_algo_name):\n",
        "    \"\"\"Helper function to visualize clusters.\"\"\"\n",
        "\n",
        "    feature_names = list(combination)\n",
        "    n_features = len(combination)\n",
        "\n",
        "    # Get cluster centers - they are in the scaled space\n",
        "    if cluster_algo_name == 'kmeans':\n",
        "        scaled_centers = clusterer.cluster_centers_\n",
        "    elif cluster_algo_name == 'gmm':\n",
        "        scaled_centers = clusterer.means_\n",
        "    else:\n",
        "        print(f\"Warning: Unknown cluster algorithm '{cluster_algo_name}' for plotting.\")\n",
        "        return\n",
        "\n",
        "    # Inverse transform centers back to the original scale\n",
        "    cluster_centers = scaler.inverse_transform(scaled_centers)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    if n_features == 1:\n",
        "        feature = feature_names[0]\n",
        "        # Plot histograms for the original data\n",
        "        plt.hist(df_outliers[feature], bins=20, alpha=0.5, label='Outliers', color='red')\n",
        "        plt.hist(df_non_outliers[feature], bins=20, alpha=0.5, label='Non-Outliers', color='blue')\n",
        "        plt.title(f'{cluster_algo_name.capitalize()} Distribution of {feature} for Outliers and Non-Outliers')\n",
        "        plt.xlabel(feature)\n",
        "        plt.ylabel('Frequency')\n",
        "\n",
        "        # Plot inverse-transformed cluster centers as vertical lines\n",
        "        for i, center in enumerate(cluster_centers):\n",
        "             plt.axvline(center[0], color='black', linestyle='--', label=f'Cluster Center {i}') # Add label per center\n",
        "\n",
        "    elif n_features == 2:\n",
        "        feat1, feat2 = feature_names[0], feature_names[1]\n",
        "\n",
        "        # Scatter plot for outliers, colored by cluster label\n",
        "        plt.scatter(df_outliers[df_outliers['cluster'] == 0][feat1],\n",
        "                    df_outliers[df_outliers['cluster'] == 0][feat2],\n",
        "                    label='Outliers Cluster 0', marker='o', color='red', alpha=0.7)\n",
        "        plt.scatter(df_outliers[df_outliers['cluster'] == 1][feat1],\n",
        "                    df_outliers[df_outliers['cluster'] == 1][feat2],\n",
        "                    label='Outliers Cluster 1', marker='x', color='red', alpha=0.7) # Using 'x' for cluster 1\n",
        "\n",
        "        # Scatter plot for non-outliers, colored by cluster label\n",
        "        plt.scatter(df_non_outliers[df_non_outliers['cluster'] == 0][feat1],\n",
        "                    df_non_outliers[df_non_outliers['cluster'] == 0][feat2],\n",
        "                    label='Non-Outliers Cluster 0', marker='o', color='blue', alpha=0.7)\n",
        "        plt.scatter(df_non_outliers[df_non_outliers['cluster'] == 1][feat1],\n",
        "                    df_non_outliers[df_non_outliers['cluster'] == 1][feat2],\n",
        "                    label='Non-Outliers Cluster 1', marker='x', color='blue', alpha=0.7) # Using 'x' for cluster 1\n",
        "\n",
        "        # Plot inverse-transformed cluster centers\n",
        "        plt.scatter(cluster_centers[:, 0], cluster_centers[:, 1], marker='*', s=200, c='black', label='Cluster centers')\n",
        "\n",
        "        plt.title(f'{cluster_algo_name.capitalize()} Clustering for {feat1} and {feat2}')\n",
        "        plt.xlabel(feat1)\n",
        "        plt.ylabel(feat2)\n",
        "\n",
        "    # Add legend and show plot\n",
        "    plt.legend()\n",
        "    plt.grid(True) # Optional: Add grid\n",
        "    plt.show()\n",
        "\n",
        "def dynamic_clustering(df_features, df_outliers, df_non_outliers, cluster_type='kmeans', max_combination_size=2):\n",
        "    \"\"\"\n",
        "    Performs dynamic clustering on combinations of features for outlier and non-outlier data,\n",
        "    and visualizes the results.\n",
        "\n",
        "    Args:\n",
        "        df_features (pd.DataFrame): The DataFrame containing the features to be combined.\n",
        "                                     This should typically include only the relevant features.\n",
        "        df_outliers (pd.DataFrame): DataFrame containing outlier data. Must contain the features\n",
        "                                    present in df_features.\n",
        "        df_non_outliers (pd.DataFrame): DataFrame containing non-outlier data. Must contain the\n",
        "                                        features present in df_features.\n",
        "        cluster_type (str): Type of clustering algorithm ('kmeans' or 'gmm'). Defaults to 'kmeans'.\n",
        "        max_combination_size (int): Maximum size of feature combinations to consider. Defaults to 2.\n",
        "    \"\"\"\n",
        "    if cluster_type.lower() not in ['kmeans', 'gmm']:\n",
        "        print(f\"Error: Invalid cluster_type '{cluster_type}'. Must be 'kmeans' or 'gmm'.\")\n",
        "        return\n",
        "\n",
        "    # Generate feature combinations from the specified feature dataframe\n",
        "    combinations = feature_combinations(df_features, max_combination_size)\n",
        "\n",
        "    if not combinations:\n",
        "        print(\"No feature combinations generated. Check input dataframe and max_combination_size.\")\n",
        "        return\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "\n",
        "    # Keep track of original indices to correctly assign clusters back\n",
        "    outlier_indices = df_outliers.index\n",
        "    non_outlier_indices = df_non_outliers.index\n",
        "\n",
        "    for combination in combinations:\n",
        "        print(f\"\\nProcessing combination: {combination} using {cluster_type.upper()}\")\n",
        "\n",
        "        # Select data for the current combination - ensure features exist in both dataframes\n",
        "        try:\n",
        "            data_outliers_subset = df_outliers[list(combination)].copy()\n",
        "            data_non_outliers_subset = df_non_outliers[list(combination)].copy()\n",
        "        except KeyError as e:\n",
        "            print(f\"Skipping combination {combination}: Feature {e} not found in one of the dataframes.\")\n",
        "            continue\n",
        "\n",
        "        # Combine data for scaling and clustering\n",
        "        all_data = pd.concat([data_outliers_subset, data_non_outliers_subset])\n",
        "        if all_data.isnull().sum().sum() > 0:\n",
        "            print(f\"  -> Handling {all_data.isnull().sum().sum()} missing values in this combination...\")\n",
        "            all_data_imputed = imputer.fit_transform(all_data)\n",
        "        else:\n",
        "            print(\"  -> No missing values found in this combination.\")\n",
        "            all_data_imputed = all_data.values # Convert to numpy array if no imputation happened\n",
        "        # Convert timedelta columns to total seconds before scaling\n",
        "\n",
        "\n",
        "        # Preprocess data\n",
        "        scaler = StandardScaler()\n",
        "        all_data_scaled = scaler.fit_transform(all_data_imputed) # Fit on combined data for this combination\n",
        "\n",
        "        # Perform clustering\n",
        "        if cluster_type.lower() == 'kmeans':\n",
        "            # Instantiate KMeans inside the loop as it's specific to the data subset\n",
        "            clusterer = KMeans(n_clusters=2, random_state=42, n_init=10) # Added random_state and n_init\n",
        "            all_data_clustered = clusterer.fit_predict(all_data_scaled)\n",
        "            algo_name_for_plot = 'kmeans'\n",
        "        elif cluster_type.lower() == 'gmm':\n",
        "            # --- FIX Attempt: Change covariance_type ---\n",
        "            try:\n",
        "                # Try with 'full' first (most flexible)\n",
        "                clusterer = GaussianMixture(n_components=2, random_state=42, covariance_type='full')\n",
        "                clusterer.fit(all_data_scaled)\n",
        "                print(\"  -> GMM fit with 'full' covariance type successful.\")\n",
        "            except ValueError: # Catch potential errors from 'full' like singular covariance\n",
        "                print(\"  -> GMM fit with 'full' failed. Trying 'diag' covariance type...\")\n",
        "                try:\n",
        "                    # If 'full' fails, try 'diag' (diagonal covariance)\n",
        "                    clusterer = GaussianMixture(n_components=2, random_state=42, covariance_type='diag')\n",
        "                    clusterer.fit(all_data_scaled)\n",
        "                    print(\"  -> GMM fit with 'diag' covariance type successful.\")\n",
        "                except Exception as e: # Catch any other errors\n",
        "                    print(f\"  -> GMM fit with 'diag' also failed: {e}\")\n",
        "                    print(f\"  -> Skipping GMM clustering for combination {combination} due to fitting issues.\")\n",
        "                    # Skip plotting for this combination if fit fails\n",
        "                    continue # Move to the next combination in the loop\n",
        "\n",
        "            # If we reached here, fit was successful with either 'full' or 'diag'\n",
        "            all_data_clustered = clusterer.predict(all_data_scaled)\n",
        "            algo_name_for_plot = 'gmm'\n",
        "\n",
        "        # Assign cluster labels back to original dataframes using indices\n",
        "        # This is a temporary assignment for plotting within this loop iteration.\n",
        "        # Create temporary series to align with original indices\n",
        "        outlier_clusters = pd.Series(all_data_clustered[:len(data_outliers_subset)], index=data_outliers_subset.index)\n",
        "        non_outlier_clusters = pd.Series(all_data_clustered[len(data_outliers_subset):], index=data_non_outliers_subset.index)\n",
        "\n",
        "        # Assign the cluster labels back to the input dataframes for plotting\n",
        "        # WARNING: This modifies the input dataframes 'df_outliers' and 'df_non_outliers'\n",
        "        # in place in each iteration. The 'cluster' column will reflect the clustering\n",
        "        # for the *current* combination being plotted.\n",
        "        df_outliers['cluster'] = outlier_clusters\n",
        "        df_non_outliers['cluster'] = non_outlier_clusters\n",
        "\n",
        "\n",
        "        # Visualize the clusters for this combination\n",
        "        plot_clusters(df_outliers, df_non_outliers, combination, clusterer, scaler, algo_name_for_plot)\n",
        "\n",
        "        # Optional: Clean up the temporary 'cluster' column if you don't want it\n",
        "        # cluttering the original dataframes after the loop finishes.\n",
        "        # del df_outliers['cluster']\n",
        "        # del df_non_outliers['cluster']\n",
        "\n",
        "\n",
        "# Example Usage (assuming you have df_features, slow_outliers, non_slow_outliers DataFrames defined)\n",
        "# Replace with your actual data loading and splitting logic.\n",
        "# For example:\n",
        "# df = pd.read_csv('your_data.csv')\n",
        "# # Assume 'is_outlier' is a column indicating outliers\n",
        "# slow_outliers = df[df['is_outlier'] == True].drop(columns=['is_outlier']).copy()\n",
        "# non_slow_outliers = df[df['is_outlier'] == False].drop(columns=['is_outlier']).copy()\n",
        "# # Define the 5 features (replace with your actual feature names)\n",
        "# feature_list = ['feature1', 'feature2', 'feature3', 'feature4', 'feature5']\n",
        "# df_features = df[feature_list].copy()\n",
        "# slow_outliers = slow_outliers[feature_list].copy()\n",
        "# non_slow_outliers = non_slow_outliers[feature_list].copy()\n",
        "\n",
        "\n",
        "# Then call the function:\n",
        "# dynamic_clustering(df_features, slow_outliers, non_slow_outliers, cluster_type='kmeans', max_combination_size=2)\n",
        "# dynamic_clustering(df_features, slow_outliers, non_slow_outliers, cluster_type='gmm', max_combination_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calling the functions to get results for single features\n",
        "# Kmeans single\n",
        "dynamic_clustering(new_df, slow_outliers_pca, non_slow_outliers_pca, cluster_type='kmeans', max_combination_size=1)\n"
      ],
      "metadata": {
        "id": "FTJJBdU0B7VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kmeans pairs\n",
        "dynamic_clustering(new_df, slow_outliers_pca, non_slow_outliers_pca, cluster_type='kmeans', max_combination_size=2)"
      ],
      "metadata": {
        "id": "sKEq7DGyFqu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GMM single\n",
        "dynamic_clustering(new_df, slow_outliers_pca, non_slow_outliers_pca, cluster_type='gmm', max_combination_size=1)"
      ],
      "metadata": {
        "id": "5Amjl2dIFtZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GMM pairs\n",
        "dynamic_clustering(new_df, slow_outliers_pca, non_slow_outliers_pca, cluster_type='gmm', max_combination_size=2)"
      ],
      "metadata": {
        "id": "75ITeRaSF1s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXdXRsSfu0hi"
      },
      "source": [
        "## Using the best laptime as a reference for best performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "383fg2pP2JIf"
      },
      "outputs": [],
      "source": [
        "weather_df = pd.read_csv('Weather.csv')\n",
        "weather_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4W234Jw_5bJ"
      },
      "outputs": [],
      "source": [
        "weather_df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4grryNg__gG"
      },
      "outputs": [],
      "source": [
        "weather_df.drop(columns=['EventName'], errors='ignore', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQQS1cFl_k0S"
      },
      "outputs": [],
      "source": [
        "weather_df['Time'] = pd.to_timedelta(weather_df['Time'])\n",
        "lap_data['Time'] = pd.to_timedelta(lap_data['Time'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUYyXXQ3BOFt"
      },
      "outputs": [],
      "source": [
        "weather_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4Hn2VybBoGd"
      },
      "outputs": [],
      "source": [
        "weather_df = weather_df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbKeoNxqB0PP"
      },
      "outputs": [],
      "source": [
        "weather_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74k1G_0RCfEU"
      },
      "outputs": [],
      "source": [
        "weather_df['RoundNumber'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOGUj89f7li8"
      },
      "outputs": [],
      "source": [
        "Time = weather_df['Time'].values\n",
        "Time[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz0H7i1C77PA"
      },
      "outputs": [],
      "source": [
        "TTime = lap_data['Time'].values\n",
        "TTime[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RbhYk_A83rk"
      },
      "source": [
        "Time scales seem to be different so need to make sure they have same reference point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zRdbrl-88_vQ"
      },
      "outputs": [],
      "source": [
        "offset = lap_data['Time'].min() - weather_df['Time'].min()\n",
        "weather_df['AdjustedTime'] = weather_df['Time'] + offset\n",
        "# using merge_asof for time based joining instead of merge to account for the time intervals the data was sampled\n",
        "lap_data = lap_data.sort_values('Time')\n",
        "weather_df = weather_df.sort_values('AdjustedTime')\n",
        "lap_weather_df = pd.merge_asof(lap_data, weather_df, left_on='Time', right_on='AdjustedTime', direction='nearest')\n",
        "lap_weather_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQXki1ocPBLf"
      },
      "outputs": [],
      "source": [
        "lap_weather_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tj3bitemX8rg"
      },
      "outputs": [],
      "source": [
        "lap_weather_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUpGZJ4-UsN3"
      },
      "outputs": [],
      "source": [
        "print(lap_weather_df['Time_x'].values[:5])\n",
        "print(lap_weather_df['Time_y'].values[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWZR10JMVJ1y"
      },
      "source": [
        "Time x seems to be more related to time recorded for lap event. Time y seems to be for matching the weather"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fuewlz8QVMsN"
      },
      "outputs": [],
      "source": [
        "# Dropping irrelevant redundant columns\n",
        "lap_weather_df.drop(columns=['Time_y', 'RoundNumber_y'], inplace=True)\n",
        "lap_weather_df.rename(columns={'Time_x': 'Time', 'RoundNumber_x': 'RoundNumber'}, inplace=True)\n",
        "lap_weather_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hOuASF22ZmUc"
      },
      "outputs": [],
      "source": [
        "lap_weather_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fOZrLn3QZxjN"
      },
      "outputs": [],
      "source": [
        "lap_weather_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6G6kMP9aCWv"
      },
      "outputs": [],
      "source": [
        "lap_weather_df = lap_weather_df.sort_values('Time')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "for driver in lap_weather_df['DriverNumber'].unique():\n",
        "  driver_data = lap_weather_df[lap_weather_df['DriverNumber'] == driver]\n",
        "  plt.plot(driver_data['LapNumber'], driver_data['TrackTemp'], marker='o', linestyle='-', label=driver, alpha=0.7)\n",
        "\n",
        "plt.xlabel('Lap Number')\n",
        "plt.ylabel('Track Temperature (C)')\n",
        "plt.title('Track Temperature across laps for each driver')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUmG6Gfyg4tb"
      },
      "source": [
        "The plot is a diagnostic to show the noise and patterns after the merge based on nearest timestamp matching. Minor matches may introduce a little noise but data still has non random behavior and no extreme behavior due to extreme outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3BOJDwzhlo3"
      },
      "outputs": [],
      "source": [
        "time_cols = ['LapTime', 'Sector1Time', 'Sector2Time', 'Sector3Time',\n",
        "             'Sector1SessionTime', 'Sector2SessionTime', 'Sector3SessionTime']\n",
        "\n",
        "for col in time_cols:\n",
        "    lap_weather_df[col] = pd.to_timedelta(lap_weather_df[col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjyZ3mVjC9g"
      },
      "outputs": [],
      "source": [
        "lap_weather_df = lap_weather_df.drop(columns=['LapStartDate', 'EventName', 'RoundNumber', 'PitOutTime', 'PitInTime', 'LapStartTime', 'AdjustedTime'], errors='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F92QolLCkN4i"
      },
      "outputs": [],
      "source": [
        "lap_weather_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHHZWyu53xQH"
      },
      "outputs": [],
      "source": [
        "# Preparing for Ensemble modeling through EDA and Feature engineering\n",
        "# EDA\n",
        "lap_weather_df['LapTimeSeconds'] = lap_weather_df['LapTime'].dt.total_seconds()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJJvusBPMC5W"
      },
      "source": [
        "Better for models because they don't handle timedelta values too well. Much better to use numerical values like seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjbZichAewwF"
      },
      "outputs": [],
      "source": [
        "lap_weather_df.drop(columns=['LapTime', 'normalized_t'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcUO0q_aMNfv"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "sns.kdeplot(data=lap_weather_df, x='LapTimeSeconds', hue='DriverNumber', fill=True, common_norm=False, alpha=0.7)\n",
        "plt.xlabel('Lap Time (seconds)')\n",
        "plt.ylabel('Density')\n",
        "plt.title('Distribution of Lap Times by Driver')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjJDxO8dP8TM"
      },
      "outputs": [],
      "source": [
        "lap_weather_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmQuTrNaVEgT"
      },
      "outputs": [],
      "source": [
        "lap_weather_df['LapTimeSeconds'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vttIv07pW70H"
      },
      "outputs": [],
      "source": [
        "# missing values\n",
        "missing_vals = lap_weather_df.isnull().mean().sort_values(ascending=False) * 100\n",
        "missing_vals[missing_vals > 0].to_frame('Missing (%)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2T5caCwZo-c"
      },
      "outputs": [],
      "source": [
        "def drop_low_missing_rows(df, threshold=0.05, exclude_columns=None):\n",
        "    \"\"\"\n",
        "    Drops rows with missing values in columns where the missing rate is below a given threshold.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input dataframe.\n",
        "        threshold (float): The max missingness allowed to drop rows (e.g., 0.05 = 5%).\n",
        "        exclude_columns (list): List of column names to exclude from consideration (e.g., ['SpeedI1', 'SpeedST']).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A cleaned dataframe with selected rows dropped.\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    if exclude_columns is None:\n",
        "        exclude_columns = []\n",
        "\n",
        "    n_rows_before = df.shape[0]\n",
        "\n",
        "    # Calculate missingness per column\n",
        "    missing_percentages = df.isnull().mean()\n",
        "\n",
        "    # Select columns with missing rate below threshold and not in exclude list\n",
        "    columns_to_consider = [\n",
        "        col for col in df.columns\n",
        "        if missing_percentages[col] <= threshold and col not in exclude_columns\n",
        "    ]\n",
        "\n",
        "    # Drop rows where any of those columns are null\n",
        "    df_cleaned = df.dropna(subset=columns_to_consider)\n",
        "\n",
        "    n_rows_after = df_cleaned.shape[0]\n",
        "    print(f\"Removed {n_rows_before - n_rows_after} rows ({((n_rows_before - n_rows_after) / n_rows_before):.2%})\")\n",
        "    print(f\"Remaining rows: {n_rows_after}\")\n",
        "\n",
        "    return df_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh8CQ6HAZt0K"
      },
      "outputs": [],
      "source": [
        "low_miss_exclude = ['SpeedI1', 'SpeedST']\n",
        "drop_low_missing_rows(lap_weather_df, threshold=0.05, exclude_columns=low_miss_exclude)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g78_sO2JZ_Ud"
      },
      "source": [
        "Removed rows for features that contain less than 5% missing data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsQqiCJtaMKc"
      },
      "outputs": [],
      "source": [
        "# group based median imputation for the high percentage missing value features\n",
        "high_missing_features = ['SpeedST', 'SpeedI1']\n",
        "for feature in high_missing_features:\n",
        "  lap_weather_df[feature] = (\n",
        "      lap_weather_df.groupby('DriverNumber')[feature].transform(lambda x: x.fillna(x.median()))\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqMZgUkPdZ3o"
      },
      "source": [
        "Very effective imputation technique that is very safe in terms of effects on model results by using medians of subgroups rather than entire data which can cause a loss of context for this type of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Etr1ELy8gdSn"
      },
      "outputs": [],
      "source": [
        "sector_cols = [\n",
        "    'Sector1Time', 'Sector2Time', 'Sector3Time',\n",
        "    'Sector1SessionTime', 'Sector2SessionTime', 'Sector3SessionTime'\n",
        "]\n",
        "\n",
        "for col in sector_cols:\n",
        "    lap_weather_df[col + '_Seconds'] = lap_weather_df[col].dt.total_seconds()\n",
        "\n",
        "lap_weather_df['Time_Seconds'] = lap_weather_df['Time'].dt.total_seconds()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NvKxMDMgfNX"
      },
      "outputs": [],
      "source": [
        "lap_weather_df.drop(columns=sector_cols, inplace=True)\n",
        "lap_weather_df.drop(columns=['Time'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZoQF7a3giIr"
      },
      "outputs": [],
      "source": [
        "numerical_features = lap_weather_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = lap_weather_df.select_dtypes(include=['object']).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tuwxAs9Kgoh3"
      },
      "outputs": [],
      "source": [
        "numerical_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0SUyj9lgrFM"
      },
      "outputs": [],
      "source": [
        "categorical_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3kL5Tj8g1ya"
      },
      "outputs": [],
      "source": [
        "# sector time violin plots\n",
        "for col in sector_cols[:3]:\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.violinplot(data=lap_weather_df, x='DriverNumber', y=col + '_Seconds')\n",
        "  plt.title(f'Distribution of {col} by Driver')\n",
        "  plt.xlabel('Driver')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TA2JryPk_-F9"
      },
      "outputs": [],
      "source": [
        "# sector session time violin plots\n",
        "for col in sector_cols[3:]:\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  sns.violinplot(data=lap_weather_df, x='DriverNumber', y=col + '_Seconds')\n",
        "  plt.title(f'Distribution of {col} by Driver')\n",
        "  plt.xlabel('Driver')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk833j-9RW5M"
      },
      "outputs": [],
      "source": [
        "# one plot per speed metric\n",
        "speed_cols = ['SpeedI1', 'SpeedI2', 'SpeedFL', 'SpeedST']\n",
        "df_melted = lap_weather_df[speed_cols].melt(var_name='SpeedType', value_name='SpeedValue')\n",
        "# Plot\n",
        "g = sns.FacetGrid(df_melted, col='SpeedType', col_wrap=2, sharex=False, sharey=False, height=4)\n",
        "g.map(sns.kdeplot, 'SpeedValue', fill=True, common_norm=False, alpha=0.7)\n",
        "g.set_titles(\"{col_name}\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhxL89xsrHQV"
      },
      "outputs": [],
      "source": [
        "weather_data = [\"TrackTemp\", \"AirTemp\", \"Humidity\", \"WindSpeed\", \"Pressure\"]\n",
        "plt.figure(figsize=(12, 6))\n",
        "for col in weather_data:\n",
        "  plt.hist(lap_weather_df[col], bins=20, alpha=0.5, label=col)\n",
        "plt.xlabel(f\"{col}\")\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Weather Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mscXZ3LnzZ_r"
      },
      "outputs": [],
      "source": [
        "lap_weather_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9ySTHFTr-1c"
      },
      "outputs": [],
      "source": [
        "# Tyre/Run dynamics data\n",
        "Tyre_track_features = ['TyreLife', 'Stint', 'TrackStatus']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osupAZmQVaqq"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Create the pie chart but without autopct\n",
        "wedges, texts = plt.pie(\n",
        "    lap_weather_df['TrackStatus'].value_counts(),\n",
        "    labels=lap_weather_df['TrackStatus'].unique(),\n",
        "    # Remove autopct here\n",
        "    textprops={'fontsize': 10},  # Adjust font size if needed\n",
        "    startangle=90,  # Optional: Rotate the start angle for better visualization\n",
        ")\n",
        "\n",
        "# Create a legend with percentages\n",
        "plt.legend(\n",
        "    wedges,\n",
        "    [f\"{l} ({s:1.1f}%)\" for l, s in zip(lap_weather_df['TrackStatus'].unique(), lap_weather_df['TrackStatus'].value_counts(normalize=True) * 100)],\n",
        "    title=\"Track Status\",\n",
        "    loc=\"center left\",\n",
        "    bbox_to_anchor=(1, 0, 0.5, 1)  # Adjust position as needed\n",
        ")\n",
        "\n",
        "plt.title('Track Status Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2-tKa2CYx5T"
      },
      "outputs": [],
      "source": [
        "plt.hist(lap_weather_df['TyreLife'], bins=20, alpha=0.5, label='Tyre Life')\n",
        "plt.xlabel('Tyre Life')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Tyre Life')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvxV2pjHY3jx"
      },
      "outputs": [],
      "source": [
        "stint_counts = lap_weather_df['Stint'].value_counts()\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(stint_counts.index, stint_counts.values)\n",
        "plt.xlabel('Stint')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Stint Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sv1VfndVoqhf"
      },
      "outputs": [],
      "source": [
        "# cat feature analysis\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=lap_weather_df, x='Compound', order=lap_weather_df['Compound'].value_counts().index)\n",
        "plt.title('Distribution of Tire compounds')\n",
        "plt.xlabel('Tire Compound')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6GeGVa9r6jc"
      },
      "outputs": [],
      "source": [
        "# encoding categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "lap_weather_df['Compound'] = le.fit_transform(lap_weather_df['Compound'])\n",
        "\n",
        "# encoding binary\n",
        "lap_weather_df['FreshTyre'] = pd.get_dummies(lap_weather_df['FreshTyre'], drop_first=True)\n",
        "lap_weather_df['Rainfall'] = pd.get_dummies(lap_weather_df['Rainfall'], drop_first=True)\n",
        "lap_weather_df['IsPersonalBest'] = pd.get_dummies(lap_weather_df['IsPersonalBest'], drop_first=True)\n",
        "lap_weather_df['IsAccurate'] = pd.get_dummies(lap_weather_df['IsAccurate'], drop_first=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OTQ457GuYAn"
      },
      "outputs": [],
      "source": [
        "# Make sure to handle potential NaNs in sector times if necessary before summing\n",
        "# For rows where all sectors are present:\n",
        "lap_weather_df = lap_weather_df.copy()\n",
        "lap_weather_df = lap_weather_df.dropna(subset=['Sector1Time_Seconds', 'Sector2Time_Seconds', 'Sector3Time_Seconds', 'LapTimeSeconds'])\n",
        "lap_weather_df['Calculated_LapTime'] = lap_weather_df['Sector1Time_Seconds'] + lap_weather_df['Sector2Time_Seconds'] + lap_weather_df['Sector3Time_Seconds']\n",
        "\n",
        "# Compare the calculated sum with the actual LapTimeSeconds\n",
        "# Check how close they are (accounting for potential float precision issues)\n",
        "difference = (lap_weather_df['Calculated_LapTime'] - lap_weather_df['LapTimeSeconds']).abs()\n",
        "print(f\"Maximum difference: {difference.max()}\")\n",
        "print(f\"Are they essentially equal? {(difference < 1e-6).all()}\") # Check if difference is negligible\n",
        "\n",
        "# Check correlation\n",
        "correlation_sum = lap_weather_df['LapTimeSeconds'].corr(lap_weather_df['Calculated_LapTime'])\n",
        "correlation_s1 = lap_weather_df['LapTimeSeconds'].corr(lap_weather_df['Sector1Time_Seconds'])\n",
        "correlation_s2 = lap_weather_df['LapTimeSeconds'].corr(lap_weather_df['Sector2Time_Seconds'])\n",
        "correlation_s3 = lap_weather_df['LapTimeSeconds'].corr(lap_weather_df['Sector3Time_Seconds'])\n",
        "print(f\"Correlation between LapTimeSeconds and Sum of Sectors: {correlation_sum}\")\n",
        "print(f\"Correlation between LapTimeSeconds and Sector1Time: {correlation_s1}\")\n",
        "print(f\"Correlation between LapTimeSeconds and Sector2Time: {correlation_s2}\")\n",
        "print(f\"Correlation between LapTimeSeconds and Sector3Time: {correlation_s3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0hMhZ76vD0z"
      },
      "source": [
        "Upon further investigation into lap session vs lap time by looking online they showed they weren't actually performance related measures. So did this correlation analysis to see if this was indeed the case and check whether they cause leakage on the target laptimeseconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lANJUfBbwJb4"
      },
      "outputs": [],
      "source": [
        "# Doing the above analysis for sector sesssion laptime\n",
        "lap_weather_df = lap_weather_df.copy()\n",
        "lap_weather_df = lap_weather_df.dropna(subset=['Sector1SessionTime_Seconds', 'Sector2SessionTime_Seconds', 'Sector3SessionTime_Seconds', 'LapTimeSeconds'])\n",
        "lap_weather_df['Calculated_LapTime'] = lap_weather_df['Sector1SessionTime_Seconds'] + lap_weather_df['Sector2SessionTime_Seconds'] + lap_weather_df['Sector3SessionTime_Seconds']\n",
        "difference = (lap_weather_df['Calculated_LapTime'] - lap_weather_df['LapTimeSeconds']).abs()\n",
        "print(f\"Maximum difference: {difference.max()}\")\n",
        "print(f\"Are they essentially equal? {(difference < 1e-6).all()}\")\n",
        "correlation_sum = lap_weather_df['LapTimeSeconds'].corr(lap_weather_df['Calculated_LapTime'])\n",
        "correlation_ss1 = lap_weather_df['LapTimeSeconds'].corr(lap_weather_df['Sector1SessionTime_Seconds'])\n",
        "correlation_ss2 = lap_weather_df['LapTimeSeconds'].corr(lap_weather_df['Sector2SessionTime_Seconds'])\n",
        "correlation_ss3 = lap_weather_df['LapTimeSeconds'].corr(lap_weather_df['Sector3SessionTime_Seconds'])\n",
        "print(f\"Correlation between LapTimeSeconds and Sum of Sectors: {correlation_sum}\")\n",
        "print(f\"Correlation between LapTimeSeconds and SectorSession1Time: {correlation_ss1}\")\n",
        "print(f\"Correlation between LapTimeSeconds and SectorSession2Time: {correlation_ss2}\")\n",
        "print(f\"Correlation between LapTimeSeconds and SectorSession3Time: {correlation_ss3}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qisQ-9yIxG-L"
      },
      "source": [
        "The same analysis for sectorsessionlaptime tells a different story so sectorsessionlaptime is actually important for the task and sectorlaptime is not. But need to do maybe some feature engineering for the time variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjosTw26yKeB"
      },
      "outputs": [],
      "source": [
        "lap_weather_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRd5N4iaAkOv"
      },
      "outputs": [],
      "source": [
        "lap_weather_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pMaF2odTAR85"
      },
      "outputs": [],
      "source": [
        "lap_weather_df = lap_weather_df.drop(columns=['Calculated_LapTime', 'Sector1Time_Seconds', 'Sector2Time_Seconds', 'Sector3Time_Seconds'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBY-hEhmGFuD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UT0Rcs-MmtOs"
      },
      "outputs": [],
      "source": [
        "def safe_data(df):\n",
        "  \"\"\"Data to not be tested until Thursday 5/1/2025. This makes sure models can be tested on unseen data\"\"\"\n",
        "  np.random.seed(42)\n",
        "  test_indices = np.random.choice(df.index, size=int(0.2 * len(df)), replace=False)\n",
        "  test_data = df.loc[test_indices]\n",
        "  remaining_data = df.drop(index=test_indices)\n",
        "  return remaining_data, test_data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "remaining_data, test_data = safe_data(lap_weather_df)\n",
        "cv_data_x = remaining_data.drop(columns=['LapTimeSeconds', 'DeltaToGlobalBest'])\n",
        "cv_data_y = remaining_data['DeltaToGlobalBest']\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "kf.get_n_splits(cv_data_x)\n",
        "\n",
        "for i, (train_index, val_index) in enumerate(kf.split(cv_data_x)):\n",
        "  print(f\"Fold {i+1}\")\n",
        "  print(f\"Fold {i+1}:\")\n",
        "  print(f\"  Train: index={train_index}\")\n",
        "  print(f\"  Val: index={val_index}\")\n",
        "  X_train, X_val = cv_data_x.iloc[train_index], cv_data_x.iloc[val_index]\n",
        "  y_train, y_val = cv_data_y.iloc[train_index], cv_data_y.iloc[val_index]"
      ],
      "metadata": {
        "id": "EyTP_p3wguNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r892mkUZlzGx"
      },
      "outputs": [],
      "source": [
        "driver_best_laptimes = lap_weather_df.groupby('DriverNumber')['LapTimeSeconds'].idxmin()\n",
        "best_laptime = lap_weather_df.loc[driver_best_laptimes]\n",
        "lap_weather_df['DeltaToGlobalBest'] = (lap_weather_df['LapTimeSeconds'] - best_laptime['LapTimeSeconds'].min()).abs()\n",
        "\n",
        "print(f\"Global best = {best_laptime['LapTimeSeconds'].min():.3f} s\")\n",
        "print(lap_weather_df['DeltaToGlobalBest'].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y8v3gCac3vsV"
      },
      "outputs": [],
      "source": [
        "test_data = safe_data()\n",
        "\n",
        "# Get the indices of the test data\n",
        "test_indices = test_data.index\n",
        "\n",
        "# Create the data for cross-validation by excluding the test data\n",
        "cv_data = lap_weather_df.drop(index=test_indices)\n",
        "\n",
        "# Now 'cv_data' contains the data to be used for cross-validation,\n",
        "# and 'test_data' remains your untouched hold-out set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o84anb9sI4xN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lMVRIyfnAzg"
      },
      "source": [
        "This is essential for effectively comparing and selecting features relative to this best laptime point. It quantifies the relative performance gap centered around zero and helps with focusing on deviations from the best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24bJevRpWTKp"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\n",
        "cv_data_x = cv_data.drop(columns=['LapTimeSeconds', 'DeltaToGlobalBest'])\n",
        "# changing the target to DeltaToGlobalBest\n",
        "cv_data_y = cv_data['DeltaToGlobalBest']\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "kf.get_n_splits(cv_data_x)\n",
        "for i, (train_index, val_index) in enumerate(kf.split(cv_data_x)):\n",
        "  print(f\"Fold {i+1}:\")\n",
        "  print(f\"  Train: index={train_index}\")\n",
        "  print(f\"  Val: index={val_index}\")\n",
        "  X_train, X_val = cv_data_x.iloc[train_index], cv_data_x.iloc[val_index]\n",
        "  y_train, y_val = cv_data_y.iloc[train_index], cv_data_y.iloc[val_index]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4z5L4g4XpXf"
      },
      "outputs": [],
      "source": [
        "# Model 1: Random Forest\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# Identify categorical and numerical features\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()  # specify categorical features\n",
        "numerical_features = X_train.select_dtypes(exclude=['object']).columns.tolist()\n",
        "\n",
        "numerical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy = 'mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_features),\n",
        "        ('cat', categorical_pipeline, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Create a pipeline with the preprocessor and the RandomForestRegressor\n",
        "rf_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
        "])\n",
        "\n",
        "rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the training data\n",
        "y_train_pred = rf_pipeline.predict(X_train)\n",
        "\n",
        "# Calculate training MSE\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "print(f\"Random Forest Training MSE: {train_mse}\")\n",
        "\n",
        "\n",
        "\n",
        "# Make predictions on the validation data\n",
        "y_pred = rf_pipeline.predict(X_val)\n",
        "\n",
        "# Calculate the mean squared error\n",
        "val_mse = mean_squared_error(y_val, y_pred)\n",
        "print(f\"Random Forest Validation MSE: {val_mse}\")  # 'mse' from previous code\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_mse_scores = []\n",
        "val_mse_scores = []\n",
        "for fold, (train_index, val_index) in enumerate(kf.split(cv_data_x)):\n",
        "  print(f\"Fold {fold + 1}\")\n",
        "  X_train, X_val = cv_data_x.iloc[train_index], cv_data_x.iloc[val_index]\n",
        "  y_train, y_val = cv_data_y.iloc[train_index], cv_data_y.iloc[val_index]\n",
        "  rf_pipeline.fit(X_train, y_train)\n",
        "\n",
        "  y_train_pred = rf_pipeline.predict(X_train)\n",
        "  train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "  train_mse_scores.append(train_mse)\n",
        "\n",
        "  y_pred = rf_pipeline.predict(X_val)\n",
        "  val_mse = mean_squared_error(y_val, y_pred)\n",
        "  val_mse_scores.append(val_mse)\n",
        "\n",
        "  print(f\"Random Forest Training MSE: {train_mse}\")\n",
        "  print(f\"Random Forest Validation MSE: {val_mse}\")\n",
        "\n",
        "print(f\"Average Training MSE: {np.mean(train_mse_scores)}\")\n",
        "print(f\"Average Validation MSE: {np.mean(val_mse_scores)}\")"
      ],
      "metadata": {
        "id": "a5-O-cGnc09_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWdxehi7rfDr"
      },
      "outputs": [],
      "source": [
        "!pip install shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uf4wthZhrb9T"
      },
      "outputs": [],
      "source": [
        "# SHAP analysis for random forest\n",
        "import shap\n",
        "explainer = shap.TreeExplainer(rf_pipeline.named_steps['regressor'])\n",
        "shap_values = explainer.shap_values(X_val)\n",
        "shap_explanation = shap.Explanation(values=shap_values, base_values=explainer.expected_value, data=X_val)\n",
        "print(f\"Expected value (Base Value): {explainer.expected_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The error \"ValueError: could not convert string to float: 'Alpine'\" is raised because the shap.TreeExplainer is trying to convert the categorical features in X_val to numerical values for calculation, but it encounters a string value ('Alpine' in this case) that it cannot convert. The issue likely stems from categorical features that were not properly encoded or were encoded in the training dataset but not in the validation dataset.\n",
        "\n",
        "The traceback shows the error originates from line 4 of ipython-input-99-1bab518d47e4 where shap_values = explainer.shap_values(X_val) is called.\n",
        "shap.TreeExplainer expects numerical input, and X_val likely contains categorical features like 'Team' or 'Driver' that have not been one-hot encoded.\n",
        "Suggested Changes\n",
        "# SHAP analysis for random forest\n",
        "import shap\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder,OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "\n",
        "# Assuming 'X_train' and 'y_train' are already defined\n",
        "\n",
        "# 1. Identify categorical and numerical features\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "numerical_features = X_train.select_dtypes(exclude=['object']).columns.tolist()\n",
        "\n",
        "\n",
        "# 2. Define pipelines for numerical and categorical features\n",
        "numerical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler()),\n",
        "])\n",
        "\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)), # Using OrdinalEncoder for SHAP\n",
        "])\n",
        "\n",
        "# 3. Create a ColumnTransformer for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_pipeline, numerical_features),\n",
        "        ('cat', categorical_pipeline, categorical_features),\n",
        "    ])\n",
        "\n",
        "# 4. Fit the preprocessor on the training data\n",
        "preprocessor.fit(X_train)  # Fit on training data to learn encoding\n",
        "\n",
        "# 5. Transform both training and validation data\n",
        "X_train_transformed = preprocessor.transform(X_train)\n",
        "X_val_transformed = preprocessor.transform(X_val)\n",
        "\n",
        "# 6. Create and train the RandomForestRegressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train_transformed, y_train)\n",
        "\n",
        "# ... (rest of your code for training and evaluation) ...\n",
        "\n",
        "# 7. SHAP analysis with transformed data\n",
        "explainer = shap.TreeExplainer(rf_regressor)\n",
        "shap_values = explainer.shap_values(X_val_transformed)  # Use transformed validation data\n",
        "shap_explanation = shap.Explanation(values=shap_values, base_values=explainer.expected_value, data=X_val_transformed)\n",
        "print(f\"Expected value (Base Value): {explainer.expected_value}\")\n",
        "Use code with caution\n",
        "Explanation of Changes:\n",
        "\n",
        "Import necessary modules: Added imports for ColumnTransformer, Pipeline, StandardScaler, OneHotEncoder, SimpleImputer, and OrdinalEncoder.\n",
        "Preprocessing Pipeline: Created a preprocessing pipeline using ColumnTransformer to apply different transformations to numerical and categorical features. Imputation is handled for missing values in both types of features. Used OrdinalEncoder instead of OneHotEncoder for categorical encoding. It encodes categories as sequential integers.\n",
        "Fit and Transform: The preprocessor is fitted on the training data (X_train) to learn the categorical encoding and scaling parameters. Both the training and validation data (X_train, X_val) are transformed using this fitted preprocessor.\n",
        "SHAP Analysis: The shap.TreeExplainer is created using the trained RandomForestRegressor, and SHAP values are calculated using the transformed validation data (X_val_transformed). This ensures that the data passed to SHAP is in a numerical format, avoiding the ValueError.\n",
        "Ordinal Encoding for SHAP: Replaced OneHotEncoder with OrdinalEncoder in the categorical pipeline. SHAP often works better with ordinal encoding as it preserves the order of categories, which can be more meaningful for interpretation.\n",
        "Sources\n",
        "Rate this answer\n",
        "I thought in my model training and validation pipeline I took care of that through the categoical_pipeline?\n",
        "\n",
        "Okay, let's address this point. You are correct that you have a categorical_pipeline in your model training and validation code (ipython-input-95-1bab518d47e4) that includes OneHotEncoder. This pipeline does handle categorical features during training and prediction within the scikit-learn framework.\n",
        "\n",
        "However, the issue arises when you use shap.TreeExplainer directly on the underlying RandomForestRegressor object (which is accessed via rf_pipeline.named_steps['regressor']).\n",
        "\n",
        "Here's the key difference:\n",
        "\n",
        "Scikit-learn Pipeline: When you use the scikit-learn pipeline for training and prediction, the OneHotEncoder within the pipeline is automatically applied to the data before it's passed to the RandomForestRegressor. This ensures the regressor receives numerical data, as required.\n",
        "\n",
        "SHAP TreeExplainer: The shap.TreeExplainer works by directly accessing the internal structure of the tree-based model (your RandomForestRegressor). It doesn't automatically use the preprocessing steps defined in your scikit-learn pipeline. Therefore, when you pass the original X_val to explainer.shap_values, it encounters the raw categorical data, leading to the ValueError.\n",
        "\n",
        "In summary: While your training and validation pipeline do handle categorical features correctly for model fitting and prediction, you need to apply the same preprocessing (specifically the categorical encoding) to the data before passing it to shap.TreeExplainer to ensure it receives numerical input.\n",
        "\n",
        "This is why the suggested changes in my previous response are necessary. They involve applying the preprocessor fitted on the training data to both the training and validation data to ensure consistency and provide the necessary encoding for SHAP analysis. I hope this clarifies the distinction!"
      ],
      "metadata": {
        "id": "d67kDhMGpx77"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLt6wnbiwflm"
      },
      "outputs": [],
      "source": [
        "# Shap visualizations\n",
        "shap.summary_plot(shap_values, X_val, plot_type=\"bar\")\n",
        "plt.title(\"SHAP Feature Importance (Mean Absolute SHAP Value)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lCqvbeGpxIin"
      },
      "outputs": [],
      "source": [
        "shap.plots.beeswarm(shap_explanation, show=False)\n",
        "plt.title(\"SHAP Feature Importance (Bee Swarm Plot)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Xcegsorxh3Y"
      },
      "source": [
        "# Interpretation:\n",
        "# - Bar Plot: Longer bars = more important features globally.\n",
        "# - Beeswarm Plot:\n",
        "#   - Wider spread = feature has a wide range of impacts.\n",
        "#   - Color (often default gradient): Red = high feature value, Blue = low feature value.\n",
        "#   - X-axis position: Positive SHAP value means the feature value pushed the prediction higher (relative to base value), Negative means it pushed it lower."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3F-aGauxikT"
      },
      "outputs": [],
      "source": [
        "shap.dependence_plot(\"Compound\", shap_values, X_val, interaction_index=\"auto\", show=False)\n",
        "plt.title(\"SHAP Dependence Plot for Compound\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3GP3txlzLDJ"
      },
      "source": [
        "# Interpretation:\n",
        "# - X-axis: Value of the feature being plotted.\n",
        "# - Y-axis: SHAP value for that feature (its impact on the prediction).\n",
        "# - Vertical scatter/patterns: Suggest interactions with other features. Color helps identify the interacting feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lI2u1oaZyqjh"
      },
      "outputs": [],
      "source": [
        "i = 0  # index in X_explain\n",
        "shap.force_plot(\n",
        "    explainer.expected_value,\n",
        "    shap_values[i],\n",
        "    X_explain.iloc[i],\n",
        "    matplotlib=True\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ho3OMQO0FE1"
      },
      "source": [
        "# Interpretation:\n",
        "# - Base value: The average prediction (explainer.expected_value).\n",
        "# - Red arrows: Features pushing the prediction higher than the base value.\n",
        "# - Blue arrows: Features pushing the prediction lower than the base value.\n",
        "# - Arrow width: Magnitude of the feature's impact for this specific prediction.\n",
        "# - Final prediction: Base value + sum of all feature contributions (SHAP values)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft57GT1S0FkO"
      },
      "outputs": [],
      "source": [
        "mean_abs_shap = abs(shap_values).mean(axis=0)\n",
        "if X_val.columns:\n",
        "  feature_importance_df = pd.DataFrame({\n",
        "      'feature' : X_val.columns,\n",
        "      'mean_abs_shap' : mean_abs_shap\n",
        "  })\n",
        "  feature_importantce_df = feature_importance_df.sort_values(by='mean_abs_shap', ascending=False)\n",
        "  print(\"\\nFeature Importance based on Mean Absolute SHAP values:\")\n",
        "  print(feature_importance_df)\n",
        "else:\n",
        "  print(\"\\nMean Absolute SHAP values (feature order corresponds to columns in X_val):\")\n",
        "  print(sorted(mean_abs_shap, reverse=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1sQxfeJ11D4"
      },
      "outputs": [],
      "source": [
        "top_n_features = 10\n",
        "if X_val.columns:\n",
        "  top_features = feature_importance_df.head(top_n_features)['feature'].head(top_n_features).tolist()\n",
        "  print(f\"\\nTop {top_n_features} Most Important Features:\")\n",
        "  print(top_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWqrDNdW3t_q"
      },
      "outputs": [],
      "source": [
        "# Model 2: XGBoost\n",
        "!pip install xgboost\n",
        "import xgboost as xgb\n",
        "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "xgb_model.fit(X_train, y_train)\n",
        "y_pred = xgb_model.predict(X_val)\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "print(f\"XGBoost MSE: {mse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTpCce115eKf"
      },
      "outputs": [],
      "source": [
        "# SHAP analysis for XGboost\n",
        "exp = shap.TreeExplainer(xgb_model)\n",
        "shap_values_xgb = exp.shap_values(X_val)\n",
        "shap_explanation = shap.Explanation(values=shap_values, base_values=exp.expected_value, data=X_val)\n",
        "print(f\"Expected value (Base Value): {exp.expected_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u98C5i2q5oRF"
      },
      "outputs": [],
      "source": [
        "# shap visualizations\n",
        "shap.summary_plot(shap_values_xgb, X_val, plot_type=\"bar\")\n",
        "plt.title(\"SHAP Feature Importance (Mean Absolute SHAP Value) XGB\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUkzqqJo528q"
      },
      "outputs": [],
      "source": [
        "shap.plots.beeswarm(shap_explanation, show=False)\n",
        "plt.title(\"SHAP Feature Importance (Bee Swarm Plot) XGB\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcRqa4eO58yR"
      },
      "outputs": [],
      "source": [
        "shap.dependence_plot(\"Compound\", shap_values_xgb, X_val, interaction_index=\"auto\", show=False)\n",
        "plt.title(\"SHAP Dependence Plot for Compound XGB\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0latm2fI6EKq"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "shap.force_plot(exp.expected_value, shap_values[i], X_explain.iloc[i], matplotlib=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4Z1DrQE7TjZ"
      },
      "outputs": [],
      "source": [
        "# SHAP feature selection\n",
        "mean_abs_shap = abs(shap_values_xgb).mean(axis=0)\n",
        "if X_val.columns:\n",
        "  feature_importance_df_xgb = pd.DataFrame({\n",
        "      'feature' : X_val.columns,\n",
        "      'mean_abs_shap' : mean_abs_shap\n",
        "  })\n",
        "  feature_importantce_df_xgb = feature_importance_df_xgb.sort_values(by='mean_abs_shap', ascending=False)\n",
        "  print(\"\\nFeature Importance based on Mean Absolute SHAP values:\")\n",
        "  print(feature_importance_df_xgb)\n",
        "else:\n",
        "  print(\"\\nMean Absolute SHAP values (feature order corresponds to columns in X_val):\")\n",
        "  print(sorted(mean_abs_shap, reverse=True))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPapAIA77-Uu"
      },
      "outputs": [],
      "source": [
        "top_n_features = 10\n",
        "if X_val.columns:\n",
        "  top_features = feature_importance_df_xgb.head(top_n_features)['feature'].head(top_n_features).tolist()\n",
        "  print(f\"\\nTop {top_n_features} Most Important Features:\")\n",
        "  print(top_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGGIWK3B9t5n"
      },
      "outputs": [],
      "source": [
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vm7cjck8KS7"
      },
      "outputs": [],
      "source": [
        "# Model 3: Catboost\n",
        "from catboost import CatBoostRegressor\n",
        "cbr_model = CatBoostRegressor(iterations=100, learning_rate=0.1, depth=6, loss_function='RMSE', random_state=42)\n",
        "cbr_model.fit(X_train, y_train)\n",
        "y_pred = cbr_model.predict(X_val)\n",
        "mse = mean_squared_error(y_val, y_pred)\n",
        "print(f\"CatBoost MSE: {mse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbocL8jC8hnm"
      },
      "outputs": [],
      "source": [
        "# SHAP analysis for catboost\n",
        "exp = shap.TreeExplainer(cbr_model)\n",
        "shap_values_cbr = exp.shap_values\n",
        "shap_explanation = shap.Explanation(values=shap_values, base_values=exp.expected_value, data=X_val)\n",
        "print(f\"Expected value (Base Value): {exp.expected_value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vb0OCBjf8ooC"
      },
      "outputs": [],
      "source": [
        "shap.summary_plot(shap_values_cbr, X_val, plot_type=\"bar\")\n",
        "plt.title(\"SHAP Feature Importance (Mean Absolute SHAP Value) CatBoost\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPDh_Qmb836p"
      },
      "outputs": [],
      "source": [
        "shap.plots.beeswarm(shap_explanation, show=False)\n",
        "plt.title(\"SHAP Feature Importance (Bee Swarm Plot) CatBoost\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9roXYle85iG"
      },
      "outputs": [],
      "source": [
        "shap.dependence_plot(\"Compound\", shap_values_cbr, X_val, interaction_index=\"auto\", show=False)\n",
        "plt.title(\"SHAP Dependence Plot for Compound CatBoost\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3ubc_nw88so"
      },
      "outputs": [],
      "source": [
        "i = 0\n",
        "shap.force_plot(exp.expected_value, shap_values[i], X_explain.iloc[i], matplotlib=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGL7wONc9DRL"
      },
      "outputs": [],
      "source": [
        "# shap feature selection\n",
        "mean_abs_shap = abs(shap_values_cbr).mean(axis=0)\n",
        "if X_val.columns:\n",
        "  feature_importance_df_cbr = pd.DataFrame({\n",
        "      'feature' : X_val.columns,\n",
        "      'mean_abs_shap' : mean_abs_shap\n",
        "  })\n",
        "  feature_importantce_df_cbr = feature_importance_df_cbr.sort_values(by='mean_abs_shap', ascending=False)\n",
        "  print(\"\\nFeature Importance based on Mean Absolute SHAP values:\")\n",
        "  print(feature_importance_df_cbr)\n",
        "else:\n",
        "  print(\"\\nMean Absolute SHAP values (feature order corresponds to columns in X_val):\")\n",
        "  print(sorted(mean_abs_shap, reverse=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eyyzd-T92pq"
      },
      "outputs": [],
      "source": [
        "top_n_features = 10\n",
        "if X_val.columns:\n",
        "  top_features = feature_importance_df_cbr.head(top_n_features)['feature'].head(top_n_features).tolist()\n",
        "  print(f\"\\nTop {top_n_features} Most Important Features:\")\n",
        "  print(top_features)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}